{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1736954838829,
     "user": {
      "displayName": "MICAELA CASTIGNETTI",
      "userId": "10426802351975039371"
     },
     "user_tz": -60
    },
    "id": "lkMFS2Zj4qMB",
    "outputId": "136cc923-968a-4bac-89d4-5990efa21980"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Projects\\Python\\personalized-information-retrieval\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from sklearn.metrics import ndcg_score, precision_recall_fscore_support\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1108,
     "status": "ok",
     "timestamp": 1736954841558,
     "user": {
      "displayName": "MICAELA CASTIGNETTI",
      "userId": "10426802351975039371"
     },
     "user_tz": -60
    },
    "id": "-n7STzaW4r4V",
    "outputId": "9b9fd849-5886-4a6b-b334-ef6a3c255df4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1736954841558,
     "user": {
      "displayName": "MICAELA CASTIGNETTI",
      "userId": "10426802351975039371"
     },
     "user_tz": -60
    },
    "id": "R9weKtiz4tJY"
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_path = \"answer_retrieval\"\n",
    "subset_answers_path = os.path.join(data_path, \"subset_answers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "executionInfo": {
     "elapsed": 8222,
     "status": "ok",
     "timestamp": 1736954849773,
     "user": {
      "displayName": "MICAELA CASTIGNETTI",
      "userId": "10426802351975039371"
     },
     "user_tz": -60
    },
    "id": "rC7JoWS34uaT",
    "outputId": "5e350e94-16b6-4a21-ea36-a65387fb3777"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>score</th>\n",
       "      <th>views</th>\n",
       "      <th>favorite</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_questions</th>\n",
       "      <th>user_answers</th>\n",
       "      <th>tags</th>\n",
       "      <th>rel_ids</th>\n",
       "      <th>rel_scores</th>\n",
       "      <th>rel_timestamps</th>\n",
       "      <th>best_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>academia_100305</td>\n",
       "      <td>What are CNRS research units and how are they ...</td>\n",
       "      <td>What are CNRS research units and how are they ...</td>\n",
       "      <td>2017-12-11 16:30:20</td>\n",
       "      <td>14</td>\n",
       "      <td>2484</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1106095</td>\n",
       "      <td>[workplace_40845, workplace_40899, workplace_9...</td>\n",
       "      <td>[travel_45926, travel_46391, travel_47403, tra...</td>\n",
       "      <td>[funding, france]</td>\n",
       "      <td>[academia_100217]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1512814966, 1513014615, 1513020822]</td>\n",
       "      <td>academia_100217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>academia_100456</td>\n",
       "      <td>Is there a free (as in freedom) alternative to...</td>\n",
       "      <td>Is there a free (as in freedom) alternative to...</td>\n",
       "      <td>2017-12-13 19:02:32</td>\n",
       "      <td>13</td>\n",
       "      <td>1117</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1106095</td>\n",
       "      <td>[workplace_40845, workplace_40899, workplace_9...</td>\n",
       "      <td>[travel_45926, travel_46391, travel_47403, tra...</td>\n",
       "      <td>[peer-review, open-access]</td>\n",
       "      <td>[academia_100462]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1513205016, 1536615064, 1553005541, 1615097827]</td>\n",
       "      <td>academia_100462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>academia_103390</td>\n",
       "      <td>Search for StackExchange citations with Google...</td>\n",
       "      <td>Search for StackExchange citations with Google...</td>\n",
       "      <td>2018-02-06 16:40:59</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1532620</td>\n",
       "      <td>[writers_27613, writers_29562, sound_42166, so...</td>\n",
       "      <td>[skeptics_39944, philosophy_3098, philosophy_9...</td>\n",
       "      <td>[citations, google-scholar]</td>\n",
       "      <td>[academia_103391]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1517936080]</td>\n",
       "      <td>academia_103391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>academia_10481</td>\n",
       "      <td>Reproducible research and corporate identity M...</td>\n",
       "      <td>Reproducible research and corporate identity</td>\n",
       "      <td>2013-06-06 09:11:05</td>\n",
       "      <td>18</td>\n",
       "      <td>372</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1106095</td>\n",
       "      <td>[academia_1698, academia_1772, academia_1911, ...</td>\n",
       "      <td>[academia_1699, academia_1700, academia_1701, ...</td>\n",
       "      <td>[copyright, creative-commons]</td>\n",
       "      <td>[academia_10499]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1370596608, 1370601095]</td>\n",
       "      <td>academia_10499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>academia_10649</td>\n",
       "      <td>Advantages of second marking In the UK a porti...</td>\n",
       "      <td>Advantages of second marking</td>\n",
       "      <td>2013-06-17 12:24:37</td>\n",
       "      <td>6</td>\n",
       "      <td>1235</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1106095</td>\n",
       "      <td>[academia_1698, academia_1772, academia_1911, ...</td>\n",
       "      <td>[academia_1699, academia_1700, academia_1701, ...</td>\n",
       "      <td>[assessment]</td>\n",
       "      <td>[academia_10650]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[1371477146, 1371477156, 1371552185]</td>\n",
       "      <td>academia_10650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                               text  \\\n",
       "0  academia_100305  What are CNRS research units and how are they ...   \n",
       "1  academia_100456  Is there a free (as in freedom) alternative to...   \n",
       "2  academia_103390  Search for StackExchange citations with Google...   \n",
       "3   academia_10481  Reproducible research and corporate identity M...   \n",
       "4   academia_10649  Advantages of second marking In the UK a porti...   \n",
       "\n",
       "                                               title           timestamp  \\\n",
       "0  What are CNRS research units and how are they ... 2017-12-11 16:30:20   \n",
       "1  Is there a free (as in freedom) alternative to... 2017-12-13 19:02:32   \n",
       "2  Search for StackExchange citations with Google... 2018-02-06 16:40:59   \n",
       "3       Reproducible research and corporate identity 2013-06-06 09:11:05   \n",
       "4                       Advantages of second marking 2013-06-17 12:24:37   \n",
       "\n",
       "   score  views  favorite  user_id  \\\n",
       "0     14   2484       2.0  1106095   \n",
       "1     13   1117       2.0  1106095   \n",
       "2      2    157       1.0  1532620   \n",
       "3     18    372       1.0  1106095   \n",
       "4      6   1235       2.0  1106095   \n",
       "\n",
       "                                      user_questions  \\\n",
       "0  [workplace_40845, workplace_40899, workplace_9...   \n",
       "1  [workplace_40845, workplace_40899, workplace_9...   \n",
       "2  [writers_27613, writers_29562, sound_42166, so...   \n",
       "3  [academia_1698, academia_1772, academia_1911, ...   \n",
       "4  [academia_1698, academia_1772, academia_1911, ...   \n",
       "\n",
       "                                        user_answers  \\\n",
       "0  [travel_45926, travel_46391, travel_47403, tra...   \n",
       "1  [travel_45926, travel_46391, travel_47403, tra...   \n",
       "2  [skeptics_39944, philosophy_3098, philosophy_9...   \n",
       "3  [academia_1699, academia_1700, academia_1701, ...   \n",
       "4  [academia_1699, academia_1700, academia_1701, ...   \n",
       "\n",
       "                            tags            rel_ids rel_scores  \\\n",
       "0              [funding, france]  [academia_100217]        [1]   \n",
       "1     [peer-review, open-access]  [academia_100462]        [1]   \n",
       "2    [citations, google-scholar]  [academia_103391]        [1]   \n",
       "3  [copyright, creative-commons]   [academia_10499]        [1]   \n",
       "4                   [assessment]   [academia_10650]        [1]   \n",
       "\n",
       "                                     rel_timestamps      best_answer  \n",
       "0              [1512814966, 1513014615, 1513020822]  academia_100217  \n",
       "1  [1513205016, 1536615064, 1553005541, 1615097827]  academia_100462  \n",
       "2                                      [1517936080]  academia_103391  \n",
       "3                          [1370596608, 1370601095]   academia_10499  \n",
       "4              [1371477146, 1371477156, 1371552185]   academia_10650  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 10000, Test examples: 100, Val examples: 100\n"
     ]
    }
   ],
   "source": [
    "# Function to load data from jsonl. this one makes everything into one dataframe. \n",
    "def load_subset_data(folder):\n",
    "    file_path = os.path.join(data_path, folder, \"subset_data.jsonl\")\n",
    "    return pd.read_json(file_path, lines=True)\n",
    "\n",
    "# Load data for train, test, and val\n",
    "train_data = load_subset_data(\"train\")\n",
    "test_data = load_subset_data(\"test\")\n",
    "val_data = load_subset_data(\"val\")\n",
    "display(train_data.head()) \n",
    "\n",
    "print(f\"Train examples: {len(train_data)}, Test examples: {len(test_data)}, Val examples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 945,
     "status": "ok",
     "timestamp": 1736954850711,
     "user": {
      "displayName": "MICAELA CASTIGNETTI",
      "userId": "10426802351975039371"
     },
     "user_tz": -60
    },
    "id": "HTgVqDq14w7o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total answers loaded: 9398\n"
     ]
    }
   ],
   "source": [
    "# Load subset_answers\n",
    "def load_subset_answers() -> dict:\n",
    "    with open(subset_answers_path) as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "answers_dict: dict = load_subset_answers()\n",
    "print(f\"Total answers loaded: {len(answers_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1736954850711,
     "user": {
      "displayName": "MICAELA CASTIGNETTI",
      "userId": "10426802351975039371"
     },
     "user_tz": -60
    },
    "id": "2YzEUjqN4yVP",
    "outputId": "01eb3544-52bc-4779-fee2-1ab72511fed9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After what George was Georgetown University na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can someone explain why Garou made Saitama do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why did Madara want to resurrect if with the E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the ~/Applications directory for? I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How to make notification but no noise when tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>What are the main criteria used by the Europea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>What happened to most of Five Star Movement (M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>How did Dumbledore know what Ron saw in the Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Do the residents of Facade talk in a language ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>According to the Catholic Church, is assassina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0   After what George was Georgetown University na...\n",
       "1   Can someone explain why Garou made Saitama do ...\n",
       "2   Why did Madara want to resurrect if with the E...\n",
       "3   What is the ~/Applications directory for? I wa...\n",
       "4   How to make notification but no noise when tim...\n",
       "..                                                ...\n",
       "95  What are the main criteria used by the Europea...\n",
       "96  What happened to most of Five Star Movement (M...\n",
       "97  How did Dumbledore know what Ron saw in the Mi...\n",
       "98  Do the residents of Facade talk in a language ...\n",
       "99  According to the Catholic Church, is assassina...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract questions and answers from subset_data.jsonl\n",
    "def extract_questions_and_answers(data):\n",
    "    questions = data[['text']]\n",
    "    answers = data[['best_answer']]\n",
    "    return questions, answers\n",
    "\n",
    "train_questions, train_answers = extract_questions_and_answers(train_data)\n",
    "test_questions, test_answers = extract_questions_and_answers(test_data)\n",
    "val_questions, val_answers = extract_questions_and_answers(val_data)\n",
    "\n",
    "display(test_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1736954850712,
     "user": {
      "displayName": "MICAELA CASTIGNETTI",
      "userId": "10426802351975039371"
     },
     "user_tz": -60
    },
    "id": "_SuIj6Fr4zbT",
    "outputId": "d55641ad-16c4-4eb3-94ed-6947eab2c27c"
   },
   "outputs": [],
   "source": [
    "punctuations = list(string.punctuation)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = \"\".join(char for char in text if char not in punctuations) # Removing punctuations from the text\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "aborted",
     "timestamp": 1736954852478,
     "user": {
      "displayName": "MICAELA CASTIGNETTI",
      "userId": "10426802351975039371"
     },
     "user_tz": -60
    },
    "id": "0yjo_61u_L_w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 model successfully loaded\n"
     ]
    }
   ],
   "source": [
    "saved_bm25_model_path = os.path.join(\"models\", \"bm25_model.pkl\")\n",
    "if os.path.exists(saved_bm25_model_path):\n",
    "    with open(saved_bm25_model_path, \"rb\") as file:\n",
    "        bm25 = pickle.load(file)\n",
    "        print(\"BM25 model successfully loaded\")\n",
    "else:\n",
    "    tokenized_corpus = []\n",
    "    for key, text in answers_dict.items():\n",
    "        tokenized_corpus.append(preprocess_text(text))\n",
    "    print(f\"BM25 corpus size: {len(tokenized_corpus)}\")\n",
    "\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 retrieval function\n",
    "def bm25_retrieve(query, top_n=5):\n",
    "    tokenized_query = preprocess_text(query)\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_indices = np.argsort(scores)[::-1][:top_n]\n",
    "    top_answers = [(list(answers_dict.keys())[idx], scores[idx]) for idx in top_indices]\n",
    "    return top_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: After what George was Georgetown University named? After what George was Georgetown University named? Or was it named that because of where it is located?\n",
      "Top retrieved answers:\n",
      "Answer ID: academia_185179, Text: Georgetown University is named after the village G...\n",
      "Answer ID: skeptics_37836, Text: No, this Huff Post article* which repeats the clai...\n",
      "Answer ID: academia_16379, Text: Here is one side effect of a university having a f...\n",
      "Answer ID: movies_116156, Text: They both being from the same neighbourhood helps ...\n",
      "Answer ID: islam_17206, Text: Abul Qasim means father of Qasim which Was our pro...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_18904\\2677434993.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  query_example = test_questions.iloc[0][0]\n"
     ]
    }
   ],
   "source": [
    "# Example retrieval\n",
    "query_example = test_questions.iloc[0][0]\n",
    "top_answers = bm25_retrieve(query_example)\n",
    "\n",
    "print(\"Query:\", str(query_example))\n",
    "print(\"Top retrieved answers:\")\n",
    "for top_id, top_score in top_answers:\n",
    "    print(f\"Answer ID: {top_id}, Text: {answers_dict[top_id][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained BERT model and tokenizer\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for training\n",
    "class RankingDataset(Dataset):\n",
    "    def __init__(self, queries, documents, labels):\n",
    "        self.queries = queries\n",
    "        self.documents = documents\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.queries[idx], self.documents[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation for reranking\n",
    "def prepare_rerank_data(questions, answer_ids, bm25_scores):\n",
    "    pairs, labels = [], []\n",
    "    for query, true_id, top_ids in zip(questions, answer_ids, bm25_scores):\n",
    "        for rank, (answer_id, score) in enumerate(top_ids):\n",
    "            pairs.append((query, answers_dict[answer_id]))\n",
    "            labels.append(1 if answer_id == true_id else 0)\n",
    "    return pairs, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully 'loaded' the bm25 scores of the train queries\n"
     ]
    }
   ],
   "source": [
    "bm25_scores_path = os.path.join(\"models\", \"bm25_scores_train.pkl\")\n",
    "\n",
    "# Since it takes time to compute the bm25 scores everytime for each training data, I computed it once and stored it in a pickle file\n",
    "if os.path.exists(bm25_scores_path):\n",
    "    with open(bm25_scores_path, \"rb\") as file:\n",
    "        bm25_scores_train = pickle.load(file) \n",
    "    print(\"Successfully 'loaded' the bm25 scores of the train queries\")\n",
    "else:\n",
    "    bm25_scores_train = []\n",
    "    for query in tqdm(train_questions[\"text\"]):\n",
    "        top_answers = bm25_retrieve(query)\n",
    "        bm25_scores_train.append(top_answers)\n",
    "\n",
    "    with open(bm25_scores_path, \"wb\") as file:\n",
    "        pickle.dump(bm25_scores_train, file)\n",
    "\n",
    "    print(\"Successfully 'calculated' and 'stored' the bm25 scores of the train queries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and val data\n",
    "bm25_scores_train = bm25_scores_train\n",
    "pairs_train, labels_train = prepare_rerank_data(\n",
    "    train_data['text'], train_data['best_answer'], bm25_scores_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize pairs\n",
    "def collate_fn(batch):\n",
    "    queries, documents, labels = zip(*batch)\n",
    "    inputs = tokenizer(list(queries), list(documents), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RankingDataset(*zip(*pairs_train), labels_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Projects\\Python\\personalized-information-retrieval\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune BERT\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_18904\\3353345207.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully 'loaded' the fine_tuned_model\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(\"models\", \"fine_tuned_model.pth\")\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    print(\"Successfully 'loaded' the fine_tuned_model\")\n",
    "\n",
    "else:\n",
    "    for epoch in range(3):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader):\n",
    "            inputs, labels = batch\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs).logits.squeeze(-1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"fine_tuned_model.pth\")\n",
    "\n",
    "    print(\"Successfully 'calculated' and 'stored' the fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rerank_model(model, questions, true_answers_id, bm25_scores):\n",
    "    model.eval()\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    for query, true_id, top_ids in zip(questions, true_answers_id, bm25_scores):\n",
    "        pairs = [(query, answers_dict[answer_id]) for answer_id, _ in top_ids]\n",
    "        inputs = tokenizer(\n",
    "            [p[0] for p in pairs], [p[1] for p in pairs],\n",
    "            padding=True, truncation=True, return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = model(**inputs).logits.squeeze(-1).cpu().numpy()\n",
    "\n",
    "\n",
    "        all_preds.append(scores)\n",
    "        all_labels.append([1 if answer_id == true_id else 0 for answer_id, _ in top_ids])\n",
    "\n",
    "    y_true = [l for labels in all_labels for l in labels]\n",
    "    y_pred = [int(p > 0) for preds in all_preds for p in preds]\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        average=\"binary\"\n",
    "    )\n",
    "\n",
    "    # Correct MAP calculation\n",
    "    def average_precision(labels, preds):\n",
    "        ap = 0.0\n",
    "        relevant_count = 0\n",
    "        for i, p in enumerate(preds):\n",
    "            if labels[i] == 1:\n",
    "                relevant_count += 1\n",
    "                ap += relevant_count / (i + 1)\n",
    "        if relevant_count == 0:\n",
    "            return 0.0\n",
    "        return ap / relevant_count\n",
    "\n",
    "    map_score = sum(average_precision(labels, preds) for labels, preds in zip(all_labels, all_preds)) / len(all_labels)\n",
    "    ndcg = ndcg_score(all_labels, all_preds)\n",
    "\n",
    "    return precision, recall, map_score, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.58it/s]\n"
     ]
    }
   ],
   "source": [
    "bm25_scores_val = []\n",
    "\n",
    "for query in tqdm(val_questions[\"text\"]):\n",
    "    top_answers = bm25_retrieve(query)\n",
    "    bm25_scores_val.append(top_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8611, Recall: 0.7209, MAP: 0.7590, nDCG: 0.8202\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "precision, recall, map_score, ndcg = evaluate_rerank_model(\n",
    "    model, val_data['text'], val_data['best_answer'], bm25_scores_val\n",
    ")\n",
    "\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, MAP: {map_score:.4f}, nDCG: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>writers_1</td>\n",
       "      <td>&lt;resources&gt;&lt;first-time-author&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>writers_2</td>\n",
       "      <td>&lt;fiction&gt;&lt;grammatical-person&gt;&lt;third-person&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>writers_3</td>\n",
       "      <td>&lt;publishing&gt;&lt;novel&gt;&lt;agent&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>writers_7</td>\n",
       "      <td>&lt;fiction&gt;&lt;genre&gt;&lt;categories&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>writers_11</td>\n",
       "      <td>&lt;terminology&gt;&lt;preparation&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id                                         Tags\n",
       "0   writers_1               <resources><first-time-author>\n",
       "1   writers_2  <fiction><grammatical-person><third-person>\n",
       "2   writers_3                   <publishing><novel><agent>\n",
       "3   writers_7                 <fiction><genre><categories>\n",
       "4  writers_11                   <terminology><preparation>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(os.path.join(data_path, \"questions_with_answer.csv\"))\n",
    "df = df[[\"Id\", \"Tags\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "def personalize_results(query, tags):\n",
    "    prompt = f\"\"\"\n",
    "    Modify and expand the following query: \"{query}\". \n",
    "    With using the following relevant tags: {tags}. Only write the query as the output.\n",
    "    \"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        store=True,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: After what George was Georgetown University named? After what George was Georgetown University named? Or was it named that because of where it is located?\n",
      "----------------------\n",
      "Personalized and modfied/expanded query: \"After what George was Georgetown University named? Specifically, can you provide details about the individual or historical figure associated with its name? Additionally, was the university's name influenced by its geographical location, and if so, how does that connection play a role in its history?\" \n"
     ]
    }
   ],
   "source": [
    "first_row = test_data.iloc[0]\n",
    "tags = df[df['Id'] == first_row[\"id\"]][\"Tags\"]\n",
    "tags = str(tags.astype(\"string\")).replace(str(first_row), \"\").strip()\n",
    "query = first_row[\"text\"]\n",
    "print(\"Original query: \" + query)\n",
    "print(\"----------------------\")\n",
    "print(\"Personalized and modfied/expanded query: \" + personalize_results(query, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25_scores_test_personalized calculated and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "bm25_test_scores_path = os.path.join(\"models\", \"bm25_scores_test_personalized.pkl\")\n",
    "\n",
    "# Since we are using OpenAI's GPT API, we just test the results on a small subset of the data to not pass the limit.\n",
    "if os.path.exists(bm25_test_scores_path):\n",
    "    with open(bm25_test_scores_path, \"rb\") as file:\n",
    "        bm25_scores_test_personalized = pickle.load(file) \n",
    "    print(\"Successfully 'loaded' the bm25 personalized scores of the test queries\")\n",
    "else:\n",
    "    bm25_scores_test_personalized = []\n",
    "    for index, row in test_data.iterrows():\n",
    "        tags = df[df['Id'] == row[\"id\"]][\"Tags\"]\n",
    "        tags = str(tags).replace(str(index), \"\").strip()\n",
    "        personalized_query = personalize_results(row[\"text\"], tags)\n",
    "        top_answers = bm25_retrieve(personalized_query, top_n=3)\n",
    "        bm25_scores_test_personalized.append(top_answers)\n",
    "\n",
    "        if index == 50:\n",
    "            break\n",
    "    \n",
    "    with open(bm25_test_scores_path, \"wb\") as file:\n",
    "        pickle.dump(bm25_scores_test_personalized, file)\n",
    "\n",
    "    print(\"bm25_scores_test_personalized calculated and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9211, Recall: 0.7778, MAP: 0.8268, nDCG: 0.8581\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "personalized_precision, personalized_recall, personalized_map_score, personalized_ndcg = evaluate_rerank_model(\n",
    "    model, test_data['text'], test_data['best_answer'], bm25_scores_test_personalized\n",
    ")\n",
    "\n",
    "print(f\"Precision: {personalized_precision:.4f}, Recall: {personalized_recall:.4f}, MAP: {personalized_map_score:.4f}, nDCG: {personalized_ndcg:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1TF9_hP6wA3RNnbvUscH4-gmGNECS0g9b",
     "timestamp": 1736788793483
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
